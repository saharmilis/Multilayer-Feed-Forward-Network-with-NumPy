{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Feed Forward Network with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the specs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras 2.4.0\n",
      "WARNING:tensorflow:From <ipython-input-2-a5d124bbd50d>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU: True\n",
      "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print('Keras', keras.__version__)\n",
    "print('GPU:', tf.test.is_gpu_available()) \n",
    "print('GPU:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Functions!\n",
    "Most code online does this using pure math.  \n",
    "Devide the calculations into functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    expx = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "def accuracy(Yhat, Y):\n",
    "    return (Y.argmax(axis=1) == Yhat.argmax(axis=1)).mean()\n",
    "\n",
    "def cross_entropy(Yhat, Y):\n",
    "    ylogy = Y * np.log(Yhat)\n",
    "    return -ylogy.sum()\n",
    "\n",
    "def ReLU(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def dReLU(X):\n",
    "    return (X > 0).astype(float)\n",
    "\n",
    "def drop(X, keep_prob=1):\n",
    "    if keep_prob < 1:\n",
    "        X = X.copy() # we don't want to change X\n",
    "        keeps = np.random.rand(X.shape[1]) < keep_prob\n",
    "        # X.shape is (nsamples, nfeatures)\n",
    "        X[:, ~keeps] = 0 # ignore\n",
    "        X[:, keeps] *= (1/keep_prob) # normalize\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(n_inputs, n_outputs):\n",
    "    boundary = np.sqrt(6 / (n_inputs + n_outputs))\n",
    "    return np.random.uniform(-boundary, boundary, size=(n_inputs, n_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(Ws, X, keep_prob=1): ###\n",
    "    \n",
    "    # input layer\n",
    "    layers = [X] \n",
    "\n",
    "    # hidden layers\n",
    "    for i,w in enumerate(Ws[:-1]):\n",
    "        X = X @ w\n",
    "        X = drop(X,keep_prob)\n",
    "        X = ReLU(X)\n",
    "        layers.append(X)\n",
    "    \n",
    "    # final layer\n",
    "    X = X @ Ws[-1]\n",
    "    X = softmax(X)\n",
    "    layers.append(X)\n",
    "\n",
    "    return layers   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(Ws, X, Y, keep_prob=1): ###\n",
    "    \n",
    "    gradients = []\n",
    "    layers = feed_forward(Ws,X,keep_prob) \n",
    "    \n",
    "    # output layer\n",
    "    y_hat = layers.pop() \n",
    "    δ = y_hat - Y\n",
    "    \n",
    "    for i in range(1,len(Ws)+1):\n",
    "        z = layers.pop()\n",
    "\n",
    "        # get the gradients\n",
    "        gradients.append(z.T @ δ) \n",
    "\n",
    "        # stop when reaching input layer - to save computations\n",
    "        if i == len(Ws):\n",
    "            break\n",
    "\n",
    "        # get the layer's loss\n",
    "        δ = δ @ Ws[-i].T \n",
    "\n",
    "        # get the activation derivative\n",
    "        δ *= dReLU(z)\n",
    "\n",
    "    # reverse the order\n",
    "    gradients.reverse()\n",
    "\n",
    "    # sanity checks\n",
    "    assert len(gradients) == len(Ws), (len(gradients), len(Ws))\n",
    "    for dW, W in zip(gradients, Ws):\n",
    "        assert dW.shape == W.shape, (dW.shape, W.shape)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Ws, X, Y):\n",
    "    Yhat = predict(Ws, X)\n",
    "    return cross_entropy(Yhat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, α=0.001, β1=0.9, β2=0.999, ϵ=1e-8):\n",
    "        self.α = α\n",
    "        self.β1 = β1\n",
    "        self.β2 = β2\n",
    "        self.ϵ = ϵ\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def send(self, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [0] * len(gradients)\n",
    "        if self.v is None:\n",
    "            self.v = [0] * len(gradients)\n",
    "\n",
    "        self.t += 1\n",
    "        αt = self.α * np.sqrt(1 - self.β2**self.t) / (1 - self.β1**self.t)\n",
    "        self.m = self.average(self.m, gradients, self.β1)        \n",
    "        self.v = self.average(self.v, (g*g for g in gradients), self.β2)\n",
    "\n",
    "        updates = [-αt * mi / (np.sqrt(vi) + self.ϵ) for mi, vi in zip(self.m, self.v)]\n",
    "        for upd in updates:\n",
    "            assert np.isfinite(upd).all()\n",
    "        return updates\n",
    "    \n",
    "    def average(self,prev, curr, β):\n",
    "        return [\n",
    "            β * p + (1 - β) * c\n",
    "            for p, c\n",
    "            in zip(prev, curr)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients check by math operations (optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(Ws, X, Y, Δ=1e-5):\n",
    "    dWs = back_propagation(Ws, X, Y, keep_prob=1)\n",
    "    Ws_ = [W.copy() for W in Ws]\n",
    "\n",
    "    for i, (W_, dW_) in enumerate(zip(Ws_, dWs)):\n",
    "        print('W{}'.format(i+1))\n",
    "        for i in range(W_.shape[0]):\n",
    "            for j in range(W_.shape[1]):\n",
    "                dw = dW_[i, j]\n",
    "                W_[i,j] += Δ\n",
    "                loss1 = loss(Ws_, X, Y)\n",
    "                W_[i,j] -= 2*Δ\n",
    "                loss2 = loss(Ws_, X, Y)\n",
    "                W_[i,j] += Δ\n",
    "                dw_ = (loss1 - loss2) / (2 * Δ)\n",
    "                rel_error = abs(dw - dw_) / abs(dw + dw_)\n",
    "                if not np.isclose(dw_, dw):\n",
    "                    print(i, j, dw, dw_, rel_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Ws, X):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape((1, -1))\n",
    "    return feed_forward(Ws, X, keep_prob=1)[-1]\n",
    "\n",
    "def display_image(im):\n",
    "    plt.imshow(im.reshape((28, 28)), cmap='gray_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "def display_prediction(idx):\n",
    "    prediction = predict(Ws, X_test[idx, :]).argmax()\n",
    "    print('Prediction:', prediction)\n",
    "    print('True Label:', Y_test[idx], ' ==> ',Y_test[idx].argmax())\n",
    "    return display_image(X_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(Ws, X, Y, optimizer, batch_size=50, keep_prob=1):    \n",
    "    nsamples = X.shape[0]\n",
    "    batch = 0\n",
    "    while True:\n",
    "        # get next batch\n",
    "        start = (batch * batch_size) % nsamples\n",
    "        stop = start + batch_size\n",
    "        batch_idx = range(start, stop)\n",
    "        X_, Y_ = X[batch_idx, :], Y[batch_idx, :]\n",
    "        \n",
    "        # calculate gradients\n",
    "        gradients = back_propagation(Ws, X_, Y_, keep_prob=keep_prob) \n",
    "        \n",
    "        # calculate updates\n",
    "        ΔWs = optimizer.send(gradients) \n",
    "        \n",
    "        # apply updates\n",
    "        for W, ΔW in zip(Ws, ΔWs): \n",
    "            W += ΔW\n",
    "            \n",
    "        batch += 1\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (60000, 784))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the data for MNIST digits.\n",
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "nsamples, width, height = X_train.shape\n",
    "nfeatures = width * height\n",
    "X_train = X_train.reshape(nsamples, nfeatures)\n",
    "X_test = X_test.reshape(-1, nfeatures)\n",
    "Y_train = keras.utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.to_categorical(Y_test)\n",
    "ncats = Y_test.shape[1]\n",
    "\n",
    "Y_train.shape,X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network\n",
    "\n",
    "I can choose how many Dense Layers to use...\n",
    "\n",
    "For example:  \n",
    "input shape ==> 64 ==> 32 ==> 10 (number of labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = fully_connected(X_train.shape[1],64)\n",
    "W2 = fully_connected(64,32)\n",
    "W3 = fully_connected(32,10)\n",
    "Ws = [W1, W2, W3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train = trainer(Ws, X_train, Y_train, batch_size=batch_size, optimizer=AdamOptimizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Prediction\n",
    "with no training at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1093\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(predict(Ws, X_test), Y_test)\n",
    "print(\"Accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for batch in train:\n",
    "    if batch == (epochs * nsamples // batch_size) : \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (6000): 0.9395\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(predict(Ws, X_test), Y_test)\n",
    "print(\"Accuracy ({:d}): {:.4f}\".format(batch, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 8\n",
      "True Label: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]  ==>  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_random = np.random.randint(len(X_test))\n",
    "display_prediction(n_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 6\n",
      "True Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]  ==>  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAGuElEQVR4nO3dTYiNfx/H8TOexlNI/sjCPSukFHWwsNOIJA8pKRZSEhtbsZmFBQsLmiwUS9lIVsrIhohmJxEltxXdlIcopc69u+uO8z1jzgz/z39er+3HNXOV3i7161ynp9VqNYC/v0l/+gaAkRErhBArhBArhBArhBArhJjyK394wYIFrb6+vnG6FeDVq1eNd+/e9fxs+6VY+/r6GsPDw2NzV8APms1m281/gyGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCHEL30xFT/3/v37ct+9e3e5v337tu02ffr08tpLly6V++rVq8t90iT/XqfwNwUhxAohxAohxAohxAohxAohxAohnLOOgd7e3nKfOXNmuT979qzt1tPTU17bbDbLfe/eveU+ODhY7vPnzy93fh9PVgghVgghVgghVgghVgghVgghVgjhnHUMzJ49u9zPnDlT7lOnTm273bp1q7z227dv5X716tVyv3nzZrlXv3/t2rXltYwtT1YIIVYIIVYIIVYIIVYIIVYI4ejmN1i1alW537hxo+324MGD8tqBgYFyHxoaKvePHz+W+759+9puFy5cKK/t7+8vd36NJyuEECuEECuEECuEECuEECuEECuE6Gm1WiP+w81mszU8PDyOt8NY63QWeuLEiXKvzmE3bNhQXnvv3r1y50fNZrMxPDz80/fPerJCCLFCCLFCCLFCCLFCCLFCCLFCCJ9n/Yc7evRouXf6Ssnq+kePHpXXnjt3rtyPHTtW7vw/T1YIIVYIIVYIIVYIIVYIIVYIIVYI4Zx1gtu6deuor/3+/Xu5d/q6yb6+vlH/7m6tX7++3BcvXvyb7mTkPFkhhFghhFghhFghhFghhFghhFghhHPWv4GvX7+23V6/fl1e22m/fv16uV+5cqXcu/Hw4cNy37Vr16h/dqf3XXf6nO6cOXPK/eLFi+W+Z8+ech8PnqwQQqwQQqwQQqwQQqwQQqwQwtHNb/D8+fNyP3nyZNvt2rVrY307NBqNT58+lfvBgwfL/eXLl22348ePj+qeOvFkhRBihRBihRBihRBihRBihRBihRDOWUfg6dOn5X7gwIFy7/TViKk6fUytk1mzZpX7kiVL2m4vXrwor+323r58+VLu9+/f7+rnj4YnK4QQK4QQK4QQK4QQK4QQK4QQK4RwztpoNAYHB8u9+rxpo9FofP78udw7vRazOhPsdG1vb2+5d3rl5qJFi8r98ePHbbdO99bJ5cuXy33z5s1ttzdv3nT1uzu9wvX06dPl3uk1q+PBkxVCiBVCiBVCiBVCiBVCiBVCiBVCTJhz1uor/Lo9R+3Wli1b2m5Hjhwpr128eHG5z5w5s9zH86sL169fX+47d+4s92nTprXd5s6dO5pb+p/ly5eX+6ZNm7r6+ePBkxVCiBVCiBVCiBVCiBVCiBVCiBVCTJhz1sOHD7fduv1c5rJly8r9zJkz5V59bnPKlPqv6M6dO+VeneE2Go3Ghw8fyr0bK1asKPfqHJUfebJCCLFCCLFCCLFCCLFCCLFCiAlzdDOeOh3N7Nixo9zv3r3bdjt16lR57dDQULmPp3Xr1pX72bNnf9OdTAyerBBCrBBCrBBCrBBCrBBCrBBCrBDCOWuj/srFkej0Ss1k1VdGdjoDnj9//ljfzoTmyQohxAohxAohxAohxAohxAohxAohnLM2un8Vabeqc95u763Tq0z3799f7gMDA223pUuXjuaWGCVPVgghVgghVgghVgghVgghVgghVggxYc5ZL1682HY7f/58ee2TJ0/G+nZGrL+/v9y3b99e7itXriz3jRs3/vI98Wd4skIIsUIIsUIIsUIIsUIIsUKICXN0c+jQobZbp4+JPXv2rNxv375d7tu2bSv3hQsXtt3mzZtXXjt58uRy55/DkxVCiBVCiBVCiBVCiBVCiBVCiBVCTJhz1sqMGTPKfc2aNV3tMBY8WSGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCFET6vVGvkf7un5T6PR+Pf43Q5MeP9qtVp//Wz4pViBP8d/gyGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCHEfwEznviwYf85TQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_random = np.random.randint(len(X_test))\n",
    "display_prediction(n_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
